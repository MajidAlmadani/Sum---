{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_df = df.loc[df['road_name'].isin(['King Abdul Aziz Road','طريق خزام','King Salman Road','شارع شبه الجزيرة','أبي نصر الفارابي','العوفي','العزايزه','الأمير سعود بن عبدالله بن جلوي','Al Nadwah','بنها','تقي الدين التميمي','شارع الشيخ عبدالله بن جبرين','شارع منصور الحازمي','الأشقر'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_points = [(24.801003, 46.559929), (24.935173, 46.814415), (24.665894, 46.891503), (24.524023, 46.591166)]\n",
    "\n",
    "df['Start_point'] = df['Start_point'].apply(ast.literal_eval)\n",
    "df['End_point'] = df['End_point'].apply(ast.literal_eval)\n",
    "\n",
    "polygon = Polygon(polygon_points)\n",
    "\n",
    "def is_point_in_polygon(start_point, end_point, polygon):\n",
    "    start = Point(start_point) \n",
    "    end = Point(end_point)\n",
    "    return polygon.contains(start) or polygon.contains(end)\n",
    "\n",
    "df['inside_polygon'] = df.apply(lambda row: is_point_in_polygon(row['Start_point'], row['End_point'],polygon), axis=1)\n",
    "\n",
    "df = df[df['inside_polygon']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from shapely.geometry import Point\n",
    "# import numpy as np\n",
    "\n",
    "# # Check for missing or None values in Start_point and End_point and handle them\n",
    "# df = df.dropna(subset=['Start_point', 'End_point'])  # Drop rows where Start_point or End_point is None\n",
    "\n",
    "# # Assuming df has Start_point and End_point as tuples (latitude, longitude)\n",
    "# # Separate the latitudes and longitudes into new columns\n",
    "# df['Start_lat'] = df['Start_point'].apply(lambda x: x[0] if x is not None else np.nan)  # Extract start latitude\n",
    "# df['Start_lng'] = df['Start_point'].apply(lambda x: x[1] if x is not None else np.nan)  # Extract start longitude\n",
    "# df['End_lat'] = df['End_point'].apply(lambda x: x[0] if x is not None else np.nan)      # Extract end latitude\n",
    "# df['End_lng'] = df['End_point'].apply(lambda x: x[1] if x is not None else np.nan)      # Extract end longitude\n",
    "\n",
    "# # Check if there are any NaN values after splitting and drop those rows\n",
    "# df = df.dropna(subset=['Start_lat', 'Start_lng', 'End_lat', 'End_lng'])\n",
    "\n",
    "# # Feature engineering: midpoints between start and end points to represent the road\n",
    "# df['Mid_lat'] = (df['Start_lat'] + df['End_lat']) / 2\n",
    "# df['Mid_lng'] = (df['Start_lng'] + df['End_lng']) / 2\n",
    "\n",
    "# # Combine the midpoints, start, and end points into a feature matrix\n",
    "# X = df[['Start_lat', 'Start_lng', 'End_lat', 'End_lng', 'Mid_lat', 'Mid_lng']]\n",
    "\n",
    "# # Standardize the features before clustering\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Option 1: Use KMeans Clustering\n",
    "# kmeans = KMeans(n_clusters=100, random_state=42)\n",
    "# df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# # Option 2: Use DBSCAN for irregularly shaped clusters (uncomment if you prefer DBSCAN)\n",
    "# # dbscan = DBSCAN(eps=0.8, min_samples=3)  # Adjust eps and min_samples for your data\n",
    "# # df['cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# # Remove clusters with fewer than 10 points\n",
    "# cluster_counts = df['cluster'].value_counts()\n",
    "# valid_clusters = cluster_counts[cluster_counts >= 2].index\n",
    "# df_filtered = df[df['cluster'].isin(valid_clusters)]\n",
    "\n",
    "# # Select one random road from each valid cluster\n",
    "# representative_roads = df_filtered.groupby('cluster').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# # Ensure the final selection contains 100 roads (if more, take random 100; if less, return the available)\n",
    "# top_100_roads = representative_roads.sample(n=100, random_state=42) if len(representative_roads) >= 100 else representative_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Check for missing or None values in Start_point and End_point and handle them\n",
    "# df = df.dropna(subset=['Start_point', 'End_point'])  # Drop rows where Start_point or End_point is None\n",
    "\n",
    "# # Assuming df has Start_point and End_point as tuples (latitude, longitude)\n",
    "# # Separate the latitudes and longitudes into new columns\n",
    "# df['Start_lat'] = df['Start_point'].apply(lambda x: x[0] if x is not None else np.nan)  # Extract start latitude\n",
    "# df['Start_lng'] = df['Start_point'].apply(lambda x: x[1] if x is not None else np.nan)  # Extract start longitude\n",
    "# df['End_lat'] = df['End_point'].apply(lambda x: x[0] if x is not None else np.nan)      # Extract end latitude\n",
    "# df['End_lng'] = df['End_point'].apply(lambda x: x[1] if x is not None else np.nan)      # Extract end longitude\n",
    "\n",
    "# # Check if there are any NaN values after splitting and drop those rows\n",
    "# df = df.dropna(subset=['Start_lat', 'Start_lng', 'End_lat', 'End_lng'])\n",
    "\n",
    "# # Feature engineering: midpoints between start and end points to represent the road\n",
    "# df['Mid_lat'] = (df['Start_lat'] + df['End_lat']) / 2\n",
    "# df['Mid_lng'] = (df['Start_lng'] + df['End_lng']) / 2\n",
    "\n",
    "# # Combine the midpoints, start, and end points into a feature matrix\n",
    "# X = df[['Start_lat', 'Start_lng', 'End_lat', 'End_lng', 'Mid_lat', 'Mid_lng']]\n",
    "\n",
    "# # Standardize the features before clustering\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Option 1: Use KMeans Clustering\n",
    "# kmeans = KMeans(n_clusters=100, random_state=42)\n",
    "# df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# # Option 2: Use DBSCAN for irregularly shaped clusters (uncomment if you prefer DBSCAN)\n",
    "# # dbscan = DBSCAN(eps=0.8, min_samples=3)  # Adjust eps and min_samples for your data\n",
    "# # df['cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# # Remove clusters with fewer than 2 points\n",
    "# cluster_counts = df['cluster'].value_counts()\n",
    "# valid_clusters = cluster_counts[cluster_counts >= 2].index\n",
    "# df_filtered = df[df['cluster'].isin(valid_clusters)]\n",
    "\n",
    "# # Select the road with the highest distance from each valid cluster\n",
    "# representative_roads = df_filtered.loc[df_filtered.groupby('cluster')['distance_km'].idxmax()]\n",
    "\n",
    "# # Ensure the final selection contains 100 roads (if more, take random 100; if less, return the available)\n",
    "# top_100_roads_2 = representative_roads.sample(n=100, random_state=42) if len(representative_roads) >= 100 else representative_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "# Assuming Start_point and End_point are tuples of (lat, lon)\n",
    "data['Start_lat'] = data['Start_point'].apply(lambda x: x[0])  # Extract start latitude from tuple\n",
    "data['Start_lon'] = data['Start_point'].apply(lambda x: x[1])  # Extract start longitude from tuple\n",
    "data['End_lat'] = data['End_point'].apply(lambda x: x[0])      # Extract end latitude from tuple\n",
    "data['End_lon'] = data['End_point'].apply(lambda x: x[1])      # Extract end longitude from tuple\n",
    "\n",
    "# Sort by distance\n",
    "data_sorted = data.sort_values(by='distance_km', ascending=False)\n",
    "\n",
    "# Function to calculate geodesic distance between two rows\n",
    "def calculate_geodesic_distance(row1, row2):\n",
    "    start1 = (row1['Start_lat'], row1['Start_lon'])\n",
    "    end1 = (row1['End_lat'], row1['End_lon'])\n",
    "    start2 = (row2['Start_lat'], row2['Start_lon'])\n",
    "    end2 = (row2['End_lat'], row2['End_lon'])\n",
    "\n",
    "    return geodesic(start1, start2).kilometers + geodesic(end1, end2).kilometers\n",
    "\n",
    "filtered_streets = []\n",
    "distance_threshold_km = 10  \n",
    "while len(filtered_streets) < 100:\n",
    "    temp_streets = []\n",
    "    for i, row in data_sorted.iterrows():\n",
    "        is_far_enough = True\n",
    "        for selected_row in filtered_streets + temp_streets:\n",
    "            if calculate_geodesic_distance(row, selected_row) < distance_threshold_km:\n",
    "                is_far_enough = False\n",
    "                break\n",
    "        if is_far_enough:\n",
    "            temp_streets.append(row)\n",
    "        if len(filtered_streets + temp_streets) >= 100:\n",
    "            break\n",
    "\n",
    "    filtered_streets.extend(temp_streets)\n",
    "\n",
    "    if len(filtered_streets) < 100:\n",
    "        distance_threshold_km -= 1 \n",
    "\n",
    "    if distance_threshold_km <= 0:\n",
    "        break\n",
    "\n",
    "filtered_data = pd.DataFrame(filtered_streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([added_df, filtered_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['inside_polygon','Start_lat','Start_lon','End_lat','End_lon'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = '/Users/cornflex/Desktop/Project/T5/T5-CapstoneProject/WebScraping/Filter_Roads/start/Dataset/'\n",
    "\n",
    "# df.to_csv(directory+'cleaned_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
